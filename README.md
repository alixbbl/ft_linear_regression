# FT_LINEAR_REGRESSION

ğŸ‘‰ POUR LANCER LE PROGRAMME TRAINING :
    'python -m programs.training ./data/data.csv'

ğŸ‘‰ POUR LANCER LE PROGRAMME PREDICTION :
    'python -m programs.prediction ./data/data.csv'

## La fonction model(): 


## Les fonctions de couts : 

La MSE (Erreur Quadratique Moyenne) comme la MAE (Mean Absolute Error) sont des fonctions
de couts classiques, utilisees pour la mesure de l'erreur entre les prÃ©dictions et les vraies 
valeurs.

ğŸ“Œ DiffÃ©rences entre MSE et MAE

    âœ… MSE (Mean Squared Error) :
    
    PÃ©nalise fortement les grandes erreurs (les erreurs Ã©levÃ©es sont amplifiÃ©es)
    Lissage : Courbe plus lisse, facilite la descente de gradient
    SensibilitÃ© aux outliers : âŒ TrÃ¨s sensible (les erreurs Ã©levÃ©es explosent au carrÃ©)
    Optimisation : Plus facile avec la descente de gradient (dÃ©rivÃ©e continue)
    UtilisÃ©e pour : RÃ©gression linÃ©aire classique

    âœ… MAE (Mean Absolute Error) :

    PÃ©nalise toutes les erreurs de maniÃ¨re Ã©gale
    Lissage : Pas de courbe lisse (la dÃ©rivÃ©e n'est pas continue en 0)
    SensibilitÃ© aux outliers : âœ… Plus robuste (les valeurs extrÃªmes ont moins d'impact)
    Optimisation : Plus difficile avec la descente de gradient (non-diffÃ©rentiable en 0)
    UtilisÃ©e pour : ModÃ¨les plus robustes aux outliers (ex : mÃ©diane, modÃ¨les avancÃ©s)

ğŸ“Œ RÃ©sumÃ© rapide :

MSE = Plus simple Ã  optimiser, mais sensible aux valeurs extrÃªmes.
MAE = Plus robuste, mais plus difficile Ã  minimiser avec la descente de gradient.
ğŸš€ Choix recommandÃ© pour la rÃ©gression linÃ©aire classique : MSE.


## L'algorithme de minimisation de la fonction de cout : 
ğŸ› ï¸ Comment fonctionne l'algo dans les grandes lignes :

On fixe a 0 les theta0 et theta1.
On fixe a 1000 par exemple le nombre d'iterations maximales du calcul (on ne peut pas calculer a l'infini).
On determine un "learning rate", en general, on le fixe a 0.01.
Puis :
On fait une prÃ©diction avec le modÃ¨le (Å· = XÎ¸).
On mesure lâ€™erreur avec la fonction de coÃ»t J(Î¸).
On calcule le gradient (âˆ‡J(Î¸)) pour savoir comment ajuster Î¸.
On met Ã  jour Î¸ avec Î¸ = Î¸ - Î± * gradient.
On rÃ©pÃ¨te jusquâ€™Ã  ce que J(Î¸) soit minimisÃ©.